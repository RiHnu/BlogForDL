{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers for Multilabel\r\n",
    "\r\n",
    "[BLOG](https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. 导入库\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "\r\n",
    "\r\n",
    "import pickle\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as no\r\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
    "\r\n",
    "from transformers import BertTokenizer, BertModel\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 处理数据"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 可视化数据"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATA_DIR = '/mnt/HDD2/lyp/DATASET/toxic'\r\n",
    "\r\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\r\n",
    "\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Unique comments:', df.comment_text.nunique() == df.shape[0])\r\n",
    "print('Null values: ', df.isnull().values.any())\r\n",
    "\r\n",
    "print('average sentence length: ', df.comment_text.str.split().str.len().mean())\r\n",
    "print('stdev sentence length: ', df.comment_text.str.split().str.len().std())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cols = df.columns\r\n",
    "label_cols = list(cols[2:])\r\n",
    "num_labels = len(label_cols)\r\n",
    "\r\n",
    "print('Count of 1 per label: \\n', df[label_cols].sum(), '\\n') # Label counts, may need to downsample or upsample\r\n",
    "print('Count of 0 per label: \\n', df[label_cols].eq(0).sum())\r\n",
    "\r\n",
    "\r\n",
    "df = df.sample(frac=1).reset_index(drop=True) #shuffle rows\r\n",
    "df['one_hot_labels'] = list(df[label_cols].values)\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels = list(df.one_hot_labels.values)\r\n",
    "comments = list(df.comment_text.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 构造 DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Transformers\r\n",
    "\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
    "encodings = tokenizer(comments,\r\n",
    "                      padding='max',\r\n",
    "                      max_length = 'xx')\r\n",
    "                      \r\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 构造 Valid_Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later\r\n",
    "label_counts = df.one_hot_labels.astype(str).value_counts()\r\n",
    "one_freq = label_counts[label_counts==1].keys()\r\n",
    "one_freq_idxs = sorted(list(df[df.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\r\n",
    "print('df label indices with only one instance: ', one_freq_idxs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Gathering single instance inputs to force into the training set after stratified split\r\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\r\n",
    "one_freq_token_types = [token_type_ids.pop(i) for i in one_freq_idxs]\r\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\r\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]\r\n",
    "\r\n",
    "\r\n",
    "# Use train_test_split to split our data into train and validation sets\r\n",
    "\r\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels, token_type_ids,attention_masks,\r\n",
    "                                                            random_state=2020, test_size=0.10, stratify = labels)\r\n",
    "\r\n",
    "# Add one frequency data to train data\r\n",
    "train_inputs.extend(one_freq_input_ids)\r\n",
    "train_labels.extend(one_freq_labels)\r\n",
    "train_masks.extend(one_freq_attention_masks)\r\n",
    "train_token_types.extend(one_freq_token_types)\r\n",
    "\r\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\r\n",
    "train_inputs = torch.tensor(train_inputs)\r\n",
    "train_labels = torch.tensor(train_labels)\r\n",
    "train_masks = torch.tensor(train_masks)\r\n",
    "train_token_types = torch.tensor(train_token_types)\r\n",
    "\r\n",
    "validation_inputs = torch.tensor(validation_inputs)\r\n",
    "validation_labels = torch.tensor(validation_labels)\r\n",
    "validation_masks = torch.tensor(validation_masks)\r\n",
    "validation_token_types = torch.tensor(validation_token_types)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 32\r\n",
    "\r\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\r\n",
    "train_sampler = RandomSampler(train_data)\r\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
    "\r\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\r\n",
    "validation_sampler = SequentialSampler(validation_data)\r\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\r\n",
    "\r\n",
    "with\r\n",
    "    torch.save(validation_dataloader,'validation_data_loader')\r\n",
    "\r\n",
    "with\r\n",
    "torch.save(train_dataloader,'train_data_loader')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('json38': conda)"
  },
  "interpreter": {
   "hash": "14e86ee4134135a6c5799ded25e98d38d02ac5f70b8a570d45d4930afe99f80f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}