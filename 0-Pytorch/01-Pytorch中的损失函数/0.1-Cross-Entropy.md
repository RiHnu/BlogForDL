# Cross Entropy Loss in Pytorch
[blog](https://sparrow.dev/cross-entropy-loss-in-pytorch/)
[中文博客](https://blog.csdn.net/wuliBob/article/details/104119616)
[源码理解](https://zhang-yang.medium.com/understanding-cross-entropy-implementation-in-pytorch-softmax-log-softmax-nll-cross-entropy-416a2b200e34)
单标签二值分类问题
单标签的目标分类
多标签的目标分类



x: Input
yhat: predict value
y: gt 真实值 [2,1,1,0,2]

如果 y 的编码采用 One-hot 编码， 那么使用 **F.cross_entropy** 函数

**E.g.**:

Batch_size = 5
n_class = 3  label 有三个: cls1, cls2, cls3

x.shape = [5, 3]
y = softmax(x) -> shape [5, 3] 每行的和为 1.

对于第一行：
[p11, p12, p13] -> p11 + p12 + p13 = 1

p11 是 模型对于输入 x[0], 预测其值为 cls1 的概率
p12 是 模型对于输入 x[0], 预测其值为 cls2 的概率
p13 是 模型对于输入 x[0], 预测其值为 cls3 的概率
> yhat 是 模型对输入 x 在每个类别上得到的概率 

如果 y = [1, 1, 3, 3, 2]

y0=1 表明, x[0] 的真实值是 cls1
y1=1 表明，x[1] 的真实值是 cls1
y2=3 表明，x[3] 的真实值是 cls3
                                    0    1     2
                                  cls1, cls2, cls3
对于 y[0]=2, 其真实 one-hot 编码为 [0,    0,    1]
     y[1]=1,                      [0,    1,    0]
     y[2]=1,                      [0,    1,    0]
     y[3]=0,                      [1,    0,    0]
     y[4]=2,                      [0,    0,    1]

因为 CrossEntropy 的公式可知, 只有非零元素才对 Loss 有贡献

对于预测值 yhat 的第一行而言 [p11, p12, p13]， 选取 p13 为最终预测的值 送入 CrossEntropy 函数

对于 F.cross_entropy 函数[而言](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html?highlight=cross_entropy#torch.nn.functional.cross_entropy)
- Input: shape = [Batch_size, n_class]
- target: shape = [Batch_size].  这个值为 真实值的 one-hot 编码
target = [2, 1, 1, 0, 2] -> t0=2 x[0] 的真实值为 cls3
loss = F.cross_entropy(logits, target)